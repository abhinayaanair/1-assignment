{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **THEORETICAL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Main purpose of RCNN in object detection:**\n",
    "\n",
    "RCNN (Region-based Convolutional Neural Network) focuses on object detection by generating region proposals (potential bounding boxes) and classifying them using CNNs. Its primary goal is to locate and classify objects within an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Difference between Fast RCNN and Faster RCNN:**\n",
    "- Fast RCNN improves upon RCNN by sharing computation across regions using a single forward pass, making it faster and more efficient.\n",
    "- Faster RCNN introduces a Region Proposal Network (RPN) to replace the slow selective search algorithm in Fast RCNN, making it significantly faster by integrating proposal generation directly into the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.  **How does YOLO handle object detection in real-time?**\n",
    "\n",
    "YOLO (You Only Look Once) treats object detection as a single regression problem, predicting bounding boxes and class probabilities simultaneously in a single forward pass, enabling real-time performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Explain the concept of Region Proposal Networks (RPN) in Faster RCNN.**\n",
    "\n",
    "RPNs are a fully convolutional network that generates potential object regions (proposals) directly from feature maps. It outputs bounding boxes and objectness scores, which are refined by subsequent layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **How does YOLOv9 improve upon its predecessors?**\n",
    "\n",
    "YOLOv9 introduces advancements such as a more efficient backbone network, better feature aggregation techniques, improved anchor-free mechanisms, and enhanced attention mechanisms, boosting speed and accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **What role does non-max suppression play in YOLO object detection?**\n",
    "\n",
    "Non-Max Suppression (NMS) eliminates redundant bounding boxes by keeping the one with the highest confidence score and suppressing others with significant overlap, ensuring one bounding box per detected object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. **Describe the data preparation process for training YOLOv9.**\n",
    "\n",
    "- Data collection: Gather diverse labeled images for training.\n",
    "- Annotation: Use tools like LabelImg to annotate bounding boxes and classes.\n",
    "- Normalization: Scale image pixel values.\n",
    "- Data Augmentation: Apply transformations like rotation, scaling, or flipping - to enhance dataset diversity.\n",
    "- Dataset Formatting: Prepare files in YOLO format (text files with bounding box coordinates)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. **What is the significance of anchor boxes in object detection models like YOLOv9?**\n",
    "\n",
    "Anchor boxes represent predefined shapes and sizes used to predict bounding boxes. They help in detecting objects of various scales and aspect ratios efficiently.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. **What is the key difference between YOLO and R-CNN architectures?**\n",
    "\n",
    "- YOLO: Performs object detection as a single regression task in one pass.\n",
    "- R-CNN: Uses a two-step approach, first generating region proposals and then classifying them, making it slower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. **Why is Faster RCNN considered faster than Fast RCNN?**\n",
    "\n",
    "Faster RCNN integrates the RPN for generating region proposals, eliminating the need for an external proposal generator like selective search, which reduces computation time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. **What is the role of selective search in RCNN?**\n",
    "\n",
    "Selective search generates region proposals by grouping similar pixels, which are then passed to a CNN for classification. It is computationally intensive and a bottleneck in RCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. **How does YOLOv9 handle multiple classes in object detection?**\n",
    "\n",
    "YOLOv9 outputs class probabilities for each bounding box. For every detected bounding box, it predicts the likelihood of belonging to each class, and the highest probability determines the class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. **What are the key differences between YOLOv3 and YOLOv9?**\n",
    "\n",
    "- YOLOv3: Uses Darknet-53 as the backbone and anchor-based detection.\n",
    "- YOLOv9: Incorporates advanced backbones, attention mechanisms, anchor-free techniques, and better optimization for speed and accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. **How is the loss function calculated in Faster RCNN?**\n",
    "\n",
    "The loss function in Faster RCNN is a combination of:\n",
    "\n",
    "- Classification loss: Cross-entropy for object classification.\n",
    "- Regression loss: Smooth L1 loss for bounding box refinement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. **Explain how YOLOv9 improves speed compared to earlier versions.**\n",
    "\n",
    "YOLOv9 leverages lighter and more efficient backbone networks, enhanced feature aggregation, and anchor-free mechanisms to reduce computational complexity and increase processing speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. **What are some challenges faced in training YOLOv9?**\n",
    "\n",
    "- Requires large annotated datasets.\n",
    "- Balancing detection of small and large objects.\n",
    "- Computational resource constraints for large models.\n",
    "- Overfitting in small datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18. **What is the significance of fine-tuning in YOLO?**\n",
    "\n",
    "Fine-tuning allows adapting a pre-trained YOLO model to a specific dataset, improving performance on a particular task while reducing training time and resource requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19. **What is the concept of bounding box regression in Faster RCNN?**\n",
    "\n",
    "Bounding box regression refines the coordinates of predicted bounding boxes to better align with ground truth by minimizing the difference (via regression loss)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20. **Describe how transfer learning is used in YOLO.**\n",
    "\n",
    "Transfer learning uses a pre-trained YOLO model trained on a large dataset (e.g., COCO) and fine-tunes it on a smaller, task-specific dataset by freezing initial layers and training later layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21. **What is the role of the backbone network in object detection models like YOLOv9?**\n",
    "\n",
    "The backbone network (e.g., CSPNet in YOLOv9) extracts features from the input image, which are used by subsequent layers for object detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22. **How does YOLO handle overlapping objects?**\n",
    "\n",
    "YOLO uses Non-Max Suppression (NMS) to resolve overlapping objects by retaining the bounding box with the highest confidence score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "23. **What is the importance of data augmentation in object detection?**\n",
    "\n",
    "Data augmentation increases dataset diversity, helps prevent overfitting, and improves model generalization by applying transformations like flipping, rotation, scaling, and color adjustment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24. **How is performance evaluated in YOLO-based object detection?**\n",
    "\n",
    "Performance is measured using:\n",
    "- Precision and recall.\n",
    "- Intersection over Union (IoU).\n",
    "- Mean Average Precision (mAP) for multiple classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25. **How do the computational requirements of Faster RCNN compare to those of YOLO?**\n",
    "\n",
    "Faster RCNN is computationally intensive due to its two-stage architecture, while YOLO is lightweight and faster, making it suitable for real-time applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "26. **What role do convolutional layers play in object detection with RCNN?**\n",
    "\n",
    "Convolutional layers extract spatial features from images, such as edges and textures, which are essential for region classification and bounding box prediction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "27. **How does the loss function in YOLO differ from other object detection models?**\n",
    "\n",
    "YOLO combines multiple loss components:\n",
    "- Localization loss for bounding box prediction.\n",
    "- Confidence loss for object presence.\n",
    "- Classification loss for class prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "28. **What are the key advantages of using YOLO for real-time object detection?**\n",
    "- Single-stage architecture for speed.\n",
    "- High inference speed suitable for real-time applications.\n",
    "- Simultaneous detection and classification in one pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "29. **How does Faster RCNN handle the trade-off between accuracy and speed?**\n",
    "\n",
    "Faster RCNN uses RPNs to reduce the number of proposals, improving efficiency while maintaining accuracy through multi-stage processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "30. **What is the role of the backbone network in both YOLO and Faster RCNN, and how do they differ?**\n",
    "\n",
    "- In YOLO, the backbone (e.g., CSPNet) is designed for real-time feature extraction and lightweight processing.\n",
    "- In Faster RCNN, the backbone (e.g., ResNet) focuses on extracting detailed features for accuracy over speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PRACTICAL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (8.3.48)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from ultralytics) (3.9.3)\n",
      "Requirement already satisfied: pillow>=7.1.2 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from ultralytics) (11.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from ultralytics) (1.13.1)\n",
      "Requirement already satisfied: torch>=1.8.0 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from ultralytics) (2.5.1)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from ultralytics) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from ultralytics) (4.67.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from ultralytics) (6.1.0)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from ultralytics) (2.2.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from ultralytics) (0.13.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from ultralytics) (2.0.13)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.55.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (6.4.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
      "Requirement already satisfied: filelock in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib>=3.3.0->ultralytics) (3.21.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ultralytics opencv-python numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 c:\\Users\\abhin\\Desktop\\ABHINAYA\\ASSIGNMENT\\images (1).jpeg: 416x640 1 dog, 69.4ms\n",
      "Speed: 3.0ms preprocess, 69.4ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 640)\n"
     ]
    }
   ],
   "source": [
    "# 1. How do you load and run inference on a custom image using the YOLOv8 model (labeled as YOLOv9)6\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the pretrained YOLOv8 model (YOLOv9 if applicable)\n",
    "model = YOLO('yolov8n.pt')  # Replace 'yolov8n.pt' with 'yolov9.pt' if you're using YOLOv9\n",
    "image_path=\"images (1).jpeg\"\n",
    "\n",
    "results=model.predict(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to C:\\Users\\abhin/.cache\\torch\\hub\\checkpoints\\fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
      "100%|██████████| 160M/160M [00:34<00:00, 4.83MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FasterRCNN(\n",
      "  (transform): GeneralizedRCNNTransform(\n",
      "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
      "  )\n",
      "  (backbone): BackboneWithFPN(\n",
      "    (body): IntermediateLayerGetter(\n",
      "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      (layer1): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer2): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer3): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (4): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (5): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer4): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fpn): FeaturePyramidNetwork(\n",
      "      (inner_blocks): ModuleList(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (layer_blocks): ModuleList(\n",
      "        (0-3): 4 x Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (extra_blocks): LastLevelMaxPool()\n",
      "    )\n",
      "  )\n",
      "  (rpn): RegionProposalNetwork(\n",
      "    (anchor_generator): AnchorGenerator()\n",
      "    (head): RPNHead(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): RoIHeads(\n",
      "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
      "    (box_head): TwoMLPHead(\n",
      "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (box_predictor): FastRCNNPredictor(\n",
      "      (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 2. How do you load the Faster RCNN model with a ResNet50 backbone and print its architecture\n",
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "\n",
    "# Load the Faster R-CNN model with a ResNet-50 backbone\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)  # Set 'pretrained=False' to load untrained model\n",
    "\n",
    "# Print the architecture of the model\n",
    "print(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction 1:\n",
      "Bounding Box: tensor([483.7888, 240.9229, 723.1383, 521.0302])\n",
      "Label: 1\n",
      "Score: 0.9991336464881897\n",
      "Prediction 2:\n",
      "Bounding Box: tensor([ 63.2200, 225.4734, 317.1106, 523.9548])\n",
      "Label: 1\n",
      "Score: 0.9964587092399597\n",
      "Prediction 3:\n",
      "Bounding Box: tensor([271.3429, 193.1448, 386.7447, 485.1668])\n",
      "Label: 1\n",
      "Score: 0.9936636686325073\n",
      "Prediction 4:\n",
      "Bounding Box: tensor([434.1417, 271.4573, 530.8661, 502.3597])\n",
      "Label: 1\n",
      "Score: 0.9805803298950195\n",
      "Prediction 5:\n",
      "Bounding Box: tensor([804.0786, 367.3167, 836.8406, 396.5289])\n",
      "Label: 1\n",
      "Score: 0.9457072019577026\n",
      "Prediction 6:\n",
      "Bounding Box: tensor([205.6631, 236.0242, 291.8345, 437.3893])\n",
      "Label: 1\n",
      "Score: 0.9445033073425293\n",
      "Prediction 7:\n",
      "Bounding Box: tensor([801.0987, 367.0279, 820.9108, 396.2760])\n",
      "Label: 1\n",
      "Score: 0.9244999289512634\n",
      "Prediction 8:\n",
      "Bounding Box: tensor([241.0067, 348.4763, 268.9014, 373.5356])\n",
      "Label: 47\n",
      "Score: 0.9156351685523987\n",
      "Prediction 9:\n",
      "Bounding Box: tensor([298.3379, 266.8593, 530.3373, 516.7930])\n",
      "Label: 1\n",
      "Score: 0.8817964196205139\n",
      "Prediction 10:\n",
      "Bounding Box: tensor([127.0199, 190.4895, 202.5847, 239.0357])\n",
      "Label: 1\n",
      "Score: 0.7261039614677429\n",
      "Prediction 11:\n",
      "Bounding Box: tensor([476.6993, 405.1725, 526.2247, 435.2248])\n",
      "Label: 59\n",
      "Score: 0.5410348773002625\n",
      "Prediction 12:\n",
      "Bounding Box: tensor([ 71.3349, 202.3412, 215.6817, 441.9437])\n",
      "Label: 1\n",
      "Score: 0.45676296949386597\n",
      "Prediction 13:\n",
      "Bounding Box: tensor([476.4944, 407.5236, 492.2316, 423.6331])\n",
      "Label: 59\n",
      "Score: 0.45480307936668396\n",
      "Prediction 14:\n",
      "Bounding Box: tensor([121.8475, 221.2441, 585.6193, 530.2069])\n",
      "Label: 1\n",
      "Score: 0.4189603328704834\n",
      "Prediction 15:\n",
      "Bounding Box: tensor([ 71.9535, 296.8270, 140.4144, 384.2881])\n",
      "Label: 1\n",
      "Score: 0.4073754549026489\n",
      "Prediction 16:\n",
      "Bounding Box: tensor([211.4391, 221.1030, 358.3564, 493.6268])\n",
      "Label: 1\n",
      "Score: 0.3918362557888031\n",
      "Prediction 17:\n",
      "Bounding Box: tensor([238.8319, 348.1701, 274.8605, 391.3152])\n",
      "Label: 47\n",
      "Score: 0.37416836619377136\n",
      "Prediction 18:\n",
      "Bounding Box: tensor([816.5451, 371.5419, 838.4374, 395.9034])\n",
      "Label: 1\n",
      "Score: 0.3730100393295288\n",
      "Prediction 19:\n",
      "Bounding Box: tensor([476.2342, 405.8616, 509.8649, 424.8961])\n",
      "Label: 59\n",
      "Score: 0.3371022939682007\n",
      "Prediction 20:\n",
      "Bounding Box: tensor([127.3211, 187.0593, 213.4666, 316.1553])\n",
      "Label: 1\n",
      "Score: 0.3199566602706909\n",
      "Prediction 21:\n",
      "Bounding Box: tensor([132.8522, 194.7097, 276.5972, 439.6603])\n",
      "Label: 1\n",
      "Score: 0.20806697010993958\n",
      "Prediction 22:\n",
      "Bounding Box: tensor([480.8540, 308.7050, 600.6283, 510.7568])\n",
      "Label: 1\n",
      "Score: 0.1986311674118042\n",
      "Prediction 23:\n",
      "Bounding Box: tensor([119.2701, 187.0294, 230.4345, 391.4985])\n",
      "Label: 1\n",
      "Score: 0.163031205534935\n",
      "Prediction 24:\n",
      "Bounding Box: tensor([248.1938, 349.9268, 272.4248, 366.7964])\n",
      "Label: 47\n",
      "Score: 0.10774430632591248\n",
      "Prediction 25:\n",
      "Bounding Box: tensor([285.6415, 430.9744, 758.1776, 523.9423])\n",
      "Label: 15\n",
      "Score: 0.10229451209306717\n",
      "Prediction 26:\n",
      "Bounding Box: tensor([350.7382, 258.5387, 380.4595, 358.0500])\n",
      "Label: 31\n",
      "Score: 0.09871511161327362\n",
      "Prediction 27:\n",
      "Bounding Box: tensor([284.0547, 246.2562, 710.6557, 528.7292])\n",
      "Label: 1\n",
      "Score: 0.09741677343845367\n",
      "Prediction 28:\n",
      "Bounding Box: tensor([ 70.8557, 243.0666, 157.7268, 385.4081])\n",
      "Label: 1\n",
      "Score: 0.09687153249979019\n",
      "Prediction 29:\n",
      "Bounding Box: tensor([249.7290, 352.3281, 271.0242, 377.1866])\n",
      "Label: 47\n",
      "Score: 0.09401197731494904\n",
      "Prediction 30:\n",
      "Bounding Box: tensor([454.3898, 461.1533, 479.2500, 481.9134])\n",
      "Label: 47\n",
      "Score: 0.08836175501346588\n",
      "Prediction 31:\n",
      "Bounding Box: tensor([252.0220, 348.1562, 265.3562, 363.4595])\n",
      "Label: 47\n",
      "Score: 0.08549775928258896\n",
      "Prediction 32:\n",
      "Bounding Box: tensor([172.7988, 175.9829, 408.4633, 516.4292])\n",
      "Label: 1\n",
      "Score: 0.08164860308170319\n",
      "Prediction 33:\n",
      "Bounding Box: tensor([478.9193, 405.2413, 580.4569, 507.2581])\n",
      "Label: 1\n",
      "Score: 0.0713842362165451\n",
      "Prediction 34:\n",
      "Bounding Box: tensor([245.4269, 358.9796, 266.8438, 374.9604])\n",
      "Label: 47\n",
      "Score: 0.07073578238487244\n",
      "Prediction 35:\n",
      "Bounding Box: tensor([243.3287, 347.6942, 268.8949, 362.1895])\n",
      "Label: 47\n",
      "Score: 0.06418392807245255\n",
      "Prediction 36:\n",
      "Bounding Box: tensor([337.9650, 387.2990, 948.1339, 505.6048])\n",
      "Label: 15\n",
      "Score: 0.06387165188789368\n",
      "Prediction 37:\n",
      "Bounding Box: tensor([244.1633, 349.7744, 277.1861, 390.2249])\n",
      "Label: 46\n",
      "Score: 0.06386186927556992\n",
      "Prediction 38:\n",
      "Bounding Box: tensor([198.6846, 304.2200, 246.3205, 375.4518])\n",
      "Label: 1\n",
      "Score: 0.06230463460087776\n",
      "Prediction 39:\n",
      "Bounding Box: tensor([483.0716, 405.7126, 524.3621, 434.3369])\n",
      "Label: 75\n",
      "Score: 0.06123118847608566\n",
      "Prediction 40:\n",
      "Bounding Box: tensor([ 74.3667, 299.6102, 131.0826, 385.1122])\n",
      "Label: 27\n",
      "Score: 0.06107601523399353\n",
      "Prediction 41:\n",
      "Bounding Box: tensor([185.2648, 249.6550, 263.8413, 422.9220])\n",
      "Label: 1\n",
      "Score: 0.05278986319899559\n",
      "Prediction 42:\n",
      "Bounding Box: tensor([ 989.6212,  365.7508, 1000.0000,  395.6917])\n",
      "Label: 1\n",
      "Score: 0.05143529176712036\n",
      "Prediction 43:\n",
      "Bounding Box: tensor([454.3093, 461.7376, 479.2701, 479.2150])\n",
      "Label: 77\n",
      "Score: 0.05134941637516022\n"
     ]
    }
   ],
   "source": [
    "#3. How do you perform inference on an online image using the Faster RCNN model and print the predictions?\n",
    "\n",
    "import torch\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "\n",
    "# Load the pre-trained Faster R-CNN model\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# URL of the image\n",
    "image_url = 'https://as1.ftcdn.net/v2/jpg/05/12/59/86/1000_F_512598633_WDA3L2yTL8ylYQtuz4ob7kHKNBL3T1b0.jpg'\n",
    "\n",
    "# Fetch and open the image\n",
    "response = requests.get(image_url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "\n",
    "# Define the image transform for Faster R-CNN\n",
    "transform = T.Compose([\n",
    "    T.ToTensor(),  # Convert image to tensor\n",
    "])\n",
    "\n",
    "# Apply the transformation\n",
    "img_tensor = transform(img)\n",
    "\n",
    "# Make the image batch-like (add a batch dimension)\n",
    "img_tensor = img_tensor.unsqueeze(0)\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():  # Disable gradient calculation for inference\n",
    "    prediction = model(img_tensor)\n",
    "\n",
    "# Extract predictions (bounding boxes, labels, and scores)\n",
    "boxes = prediction[0]['boxes']  # Bounding boxes\n",
    "labels = prediction[0]['labels']  # Labels\n",
    "scores = prediction[0]['scores']  # Scores for each detection\n",
    "\n",
    "# Print the predictions\n",
    "for i in range(len(boxes)):\n",
    "    print(f\"Prediction {i+1}:\")\n",
    "    print(f\"Bounding Box: {boxes[i]}\")\n",
    "    print(f\"Label: {labels[i]}\")\n",
    "    print(f\"Score: {scores[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\abhin/.cache/torch/hub/ultralytics_yolov5'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m cache_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m~/.cache/torch/hub/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Remove YOLOv5 cache\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrmtree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43multralytics_yolov5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\shutil.py:759\u001b[0m, in \u001b[0;36mrmtree\u001b[1;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[0;32m    757\u001b[0m     \u001b[38;5;66;03m# can't continue even if onerror hook returns\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 759\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_rmtree_unsafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monerror\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\shutil.py:610\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    608\u001b[0m         entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(scandir_it)\n\u001b[0;32m    609\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m--> 610\u001b[0m     \u001b[43monerror\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    611\u001b[0m     entries \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    612\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m entries:\n",
      "File \u001b[1;32mc:\\Users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\shutil.py:607\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_rmtree_unsafe\u001b[39m(path, onerror):\n\u001b[0;32m    606\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 607\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m scandir_it:\n\u001b[0;32m    608\u001b[0m             entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(scandir_it)\n\u001b[0;32m    609\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\abhin/.cache/torch/hub/ultralytics_yolov5'"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Path to the cache folder\n",
    "cache_dir = os.path.expanduser('~/.cache/torch/hub/')\n",
    "\n",
    "# Remove YOLOv5 cache\n",
    "shutil.rmtree(os.path.join(cache_dir, 'ultralytics_yolov5'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. How do you load an image and perform inference using YOLOv9, then display the detected objects with bounding boxes and class labels6\n",
    "\n",
    "#Don't know the ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# 5.  How do you display bounding boxes for the detected objects in an image using Faster RCNN?\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the pre-trained Faster R-CNN model\n",
    "model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Load the image (replace with your image file)\n",
    "img = Image.open('images (1).jpeg')\n",
    "\n",
    "# Define the transformation pipeline (normalize the image as required by the model)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert image to tensor\n",
    "])\n",
    "\n",
    "# Apply the transformation to the image\n",
    "img_tensor = transform(img).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    predictions = model(img_tensor)\n",
    "\n",
    "# Extract predictions (boxes, labels, scores)\n",
    "boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "labels = predictions[0]['labels'].cpu().numpy()\n",
    "scores = predictions[0]['scores'].cpu().numpy()\n",
    "\n",
    "# Filter out low-confidence predictions\n",
    "threshold = 0.5\n",
    "valid_boxes = boxes[scores >= threshold]\n",
    "valid_labels = labels[scores >= threshold]\n",
    "valid_scores = scores[scores >= threshold]\n",
    "\n",
    "# Convert image to OpenCV format\n",
    "img_cv = np.array(img)\n",
    "img_cv = cv2.cvtColor(img_cv, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# Draw bounding boxes and labels on the image\n",
    "for box, label, score in zip(valid_boxes, valid_labels, valid_scores):\n",
    "    x1, y1, x2, y2 = box.astype(int)\n",
    "    cv2.rectangle(img_cv, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    cv2.putText(img_cv, f'{label} {score:.2f}', (x1, y1 - 10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "\n",
    "# Display the image with bounding boxes\n",
    "cv2.imshow(\"Detected Objects\", img_cv)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6.How do you perform inference on a local image using Faster RCNN6\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Load the pre-trained Faster R-CNN model\n",
    "model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Load the image from your local path (replace 'path_to_image.jpg' with your image file)\n",
    "img_path = 'images (1).jpeg'\n",
    "img = Image.open(img_path)\n",
    "\n",
    "# Define the transformation pipeline (convert image to tensor)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert the image to a tensor\n",
    "])\n",
    "\n",
    "# Apply the transformation to the image\n",
    "img_tensor = transform(img).unsqueeze(0)  # Add a batch dimension\n",
    "\n",
    "# Perform inference (detect objects)\n",
    "with torch.no_grad():  # Disable gradient calculations since we're not training\n",
    "    predictions = model(img_tensor)\n",
    "\n",
    "# Extract predictions: boxes, labels, and scores\n",
    "boxes = predictions[0]['boxes'].cpu().numpy()  # Bounding boxes\n",
    "labels = predictions[0]['labels'].cpu().numpy()  # Class labels\n",
    "scores = predictions[0]['scores'].cpu().numpy()  # Confidence scores\n",
    "\n",
    "# Filter out boxes with low confidence (score threshold)\n",
    "threshold = 0.5\n",
    "valid_boxes = boxes[scores >= threshold]\n",
    "valid_labels = labels[scores >= threshold]\n",
    "valid_scores = scores[scores >= threshold]\n",
    "\n",
    "# Convert the image to OpenCV format for visualization\n",
    "img_cv = np.array(img)\n",
    "img_cv = cv2.cvtColor(img_cv, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# Loop through valid boxes and draw them on the image\n",
    "for box, label, score in zip(valid_boxes, valid_labels, valid_scores):\n",
    "    x1, y1, x2, y2 = box.astype(int)\n",
    "    cv2.rectangle(img_cv, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Draw bounding box\n",
    "    cv2.putText(img_cv, f'{label} {score:.2f}', (x1, y1 - 10), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)  # Add label and score\n",
    "\n",
    "# Display the image with bounding boxes\n",
    "cv2.imshow(\"Detected Objects\", img_cv)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhin\\AppData\\Local\\Temp\\ipykernel_14956\\1981033040.py:30: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# 7. How can you change the confidence threshold for YOLO object detection and filter out low-confidencem predictions\n",
    "\n",
    "# not sure about the ans\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example lists of training and validation loss (replace with your actual loss values)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Simulating training loop (replace with your actual model training)\n",
    "for epoch in range(10):  # Assuming 10 epochs for this example\n",
    "    # Simulated loss values (replace with actual loss values during training)\n",
    "    train_loss = 0.1 * epoch  # Example training loss (just a placeholder)\n",
    "    val_loss = 0.1 * epoch + 0.05  # Example validation loss (just a placeholder)\n",
    "\n",
    "    # Append the loss values to their respective lists\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "# Plot the training and validation loss curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(10), train_losses, label=\"Training Loss\", color='blue', marker='o')\n",
    "plt.plot(range(10), val_losses, label=\"Validation Loss\", color='red', marker='x')\n",
    "plt.title(\"Training and Validation Loss Curves\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhin\\AppData\\Local\\Temp\\ipykernel_14956\\4092616534.py:26: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example lists of training and validation loss (replace with your actual loss values)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Simulating training loop (replace with your actual model training)\n",
    "for epoch in range(10):  # Assuming 10 epochs for this example\n",
    "    # Simulated loss values (replace with actual loss values during training)\n",
    "    train_loss = 0.1 * epoch  # Example training loss (just a placeholder)\n",
    "    val_loss = 0.1 * epoch + 0.05  # Example validation loss (just a placeholder)\n",
    "\n",
    "    # Append the loss values to their respective lists\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "# Plot the training and validation loss curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(10), train_losses, label=\"Training Loss\", color='blue', marker='o')\n",
    "plt.plot(range(10), val_losses, label=\"Validation Loss\", color='red', marker='x')\n",
    "plt.title(\"Training and Validation Loss Curves\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\abhin\\AppData\\Local\\Temp\\ipykernel_14956\\2070785872.py:61: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "# Step 1: Load pre-trained Faster R-CNN model\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Step 2: Define the transformation for input images\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert image to tensor\n",
    "])\n",
    "\n",
    "# Step 3: Function to perform inference and display bounding boxes\n",
    "def perform_inference_on_folder(image_folder_path):\n",
    "    # Step 4: Iterate through the images in the folder\n",
    "    for img_name in os.listdir(image_folder_path):\n",
    "        img_path = os.path.join(image_folder_path, img_name)\n",
    "        \n",
    "        if img_path.endswith(('.png', '.jpg', '.jpeg')):  # Check for valid image files\n",
    "            # Step 5: Load the image\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            \n",
    "            # Step 6: Apply the transformation (Convert image to tensor)\n",
    "            img_tensor = transform(img).unsqueeze(0)  # Add batch dimension\n",
    "            \n",
    "            # Step 7: Perform inference\n",
    "            with torch.no_grad():  # Disable gradient calculation\n",
    "                prediction = model(img_tensor)\n",
    "            \n",
    "            # Step 8: Get bounding boxes and labels\n",
    "            boxes = prediction[0]['boxes']\n",
    "            labels = prediction[0]['labels']\n",
    "            scores = prediction[0]['scores']\n",
    "            \n",
    "            # Step 9: Set a confidence threshold\n",
    "            confidence_threshold = 0.5\n",
    "            filtered_boxes = boxes[scores >= confidence_threshold]\n",
    "            filtered_labels = labels[scores >= confidence_threshold]\n",
    "            filtered_scores = scores[scores >= confidence_threshold]\n",
    "            \n",
    "            # Step 10: Display the image with bounding boxes\n",
    "            draw = ImageDraw.Draw(img)\n",
    "            \n",
    "            for box, label, score in zip(filtered_boxes, filtered_labels, filtered_scores):\n",
    "                box = box.tolist()  # Convert tensor to list for drawing\n",
    "                draw.rectangle(box, outline=\"red\", width=3)  # Draw bounding box\n",
    "                \n",
    "                # Draw label text\n",
    "                label_text = f\"Label: {label.item()} Score: {score.item():.2f}\"\n",
    "                draw.text((box[0], box[1]), label_text, fill=\"red\")\n",
    "            \n",
    "            # Step 11: Show the image with bounding boxes\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "# Step 12: Define the folder path containing the images\n",
    "image_folder_path = \"img\"  # Replace with your folder path\n",
    "\n",
    "# Step 13: Call the function to perform inference on the images in the folder\n",
    "perform_inference_on_folder(image_folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhin\\AppData\\Local\\Temp\\ipykernel_14956\\1518458932.py:58: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "# Step 1: Load pre-trained Faster R-CNN model\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Step 2: Define the transformation for input images\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert image to tensor\n",
    "])\n",
    "\n",
    "# Step 3: Function to perform inference and display bounding boxes with confidence scores\n",
    "def perform_inference_on_image(image_path):\n",
    "    # Step 4: Load the image\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Step 5: Apply the transformation (Convert image to tensor)\n",
    "    img_tensor = transform(img).unsqueeze(0)  # Add batch dimension\n",
    "    \n",
    "    # Step 6: Perform inference\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        prediction = model(img_tensor)\n",
    "    \n",
    "    # Step 7: Get bounding boxes, labels, and scores\n",
    "    boxes = prediction[0]['boxes']\n",
    "    labels = prediction[0]['labels']\n",
    "    scores = prediction[0]['scores']\n",
    "    \n",
    "    # Step 8: Set a confidence threshold\n",
    "    confidence_threshold = 0.5\n",
    "    filtered_boxes = boxes[scores >= confidence_threshold]\n",
    "    filtered_labels = labels[scores >= confidence_threshold]\n",
    "    filtered_scores = scores[scores >= confidence_threshold]\n",
    "    \n",
    "    # Step 9: Draw bounding boxes and labels with confidence scores\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    \n",
    "    for box, label, score in zip(filtered_boxes, filtered_labels, filtered_scores):\n",
    "        box = box.tolist()  # Convert tensor to list for drawing\n",
    "        draw.rectangle(box, outline=\"red\", width=3)  # Draw bounding box\n",
    "        \n",
    "        # Create the label text (Label ID and confidence score)\n",
    "        label_text = f\"Label: {label.item()} Score: {score.item():.2f}\"\n",
    "        \n",
    "        # Draw the label text near the bounding box\n",
    "        draw.text((box[0], box[1] - 10), label_text, fill=\"red\")\n",
    "    \n",
    "    # Step 10: Display the image with bounding boxes and confidence scores\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Step 11: Define the image path\n",
    "image_path = \"images (1).jpeg\"  # Replace with the path to your image\n",
    "\n",
    "# Step 12: Call the function to perform inference and display bounding boxes with confidence scores\n",
    "perform_inference_on_image(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping yolov5 as it is not installed.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall yolov5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yolov5\n",
      "  Downloading yolov5-7.0.14-py37.py38.py39.py310-none-any.whl.metadata (10 kB)\n",
      "Collecting gitpython>=3.1.30 (from yolov5)\n",
      "  Using cached GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: matplotlib>=3.3 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from yolov5) (3.9.3)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from yolov5) (2.0.2)\n",
      "Requirement already satisfied: opencv-python>=4.1.1 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from yolov5) (4.10.0.84)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from yolov5) (11.0.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from yolov5) (6.1.0)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from yolov5) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from yolov5) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from yolov5) (1.13.1)\n",
      "Collecting thop>=0.1.1 (from yolov5)\n",
      "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: torch>=1.7.0 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from yolov5) (2.5.1)\n",
      "Requirement already satisfied: torchvision>=0.8.1 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from yolov5) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from yolov5) (4.67.1)\n",
      "Requirement already satisfied: ultralytics>=8.0.100 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from yolov5) (8.3.48)\n",
      "Requirement already satisfied: tensorboard>=2.4.1 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from yolov5) (2.18.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from yolov5) (2.2.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from yolov5) (0.13.2)\n",
      "Requirement already satisfied: setuptools>=65.5.1 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from yolov5) (75.1.0)\n",
      "Collecting fire (from yolov5)\n",
      "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting boto3>=1.19.1 (from yolov5)\n",
      "  Downloading boto3-1.35.77-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting sahi>=0.11.10 (from yolov5)\n",
      "  Downloading sahi-0.11.19-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting huggingface-hub<0.25.0,>=0.12.0 (from yolov5)\n",
      "  Downloading huggingface_hub-0.24.7-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting roboflow>=0.2.29 (from yolov5)\n",
      "  Downloading roboflow-1.1.49-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting botocore<1.36.0,>=1.35.77 (from boto3>=1.19.1->yolov5)\n",
      "  Downloading botocore-1.35.77-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.19.1->yolov5)\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3>=1.19.1->yolov5)\n",
      "  Downloading s3transfer-0.10.4-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython>=3.1.30->yolov5)\n",
      "  Using cached gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from huggingface-hub<0.25.0,>=0.12.0->yolov5) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from huggingface-hub<0.25.0,>=0.12.0->yolov5) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from huggingface-hub<0.25.0,>=0.12.0->yolov5) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from huggingface-hub<0.25.0,>=0.12.0->yolov5) (4.12.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from matplotlib>=3.3->yolov5) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from matplotlib>=3.3->yolov5) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from matplotlib>=3.3->yolov5) (4.55.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from matplotlib>=3.3->yolov5) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from matplotlib>=3.3->yolov5) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from matplotlib>=3.3->yolov5) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from matplotlib>=3.3->yolov5) (6.4.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from pandas>=1.1.4->yolov5) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from pandas>=1.1.4->yolov5) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from requests>=2.23.0->yolov5) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from requests>=2.23.0->yolov5) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from requests>=2.23.0->yolov5) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from requests>=2.23.0->yolov5) (2024.8.30)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.23.0->yolov5)\n",
      "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting opencv-python-headless==4.10.0.84 (from roboflow>=0.2.29->yolov5)\n",
      "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Collecting python-dotenv (from roboflow>=0.2.29->yolov5)\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: six in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from roboflow>=0.2.29->yolov5) (1.17.0)\n",
      "Collecting requests-toolbelt (from roboflow>=0.2.29->yolov5)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting filetype (from roboflow>=0.2.29->yolov5)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting opencv-python>=4.1.1 (from yolov5)\n",
      "  Downloading opencv_python-4.9.0.80-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Collecting shapely>=1.8.0 (from sahi>=0.11.10->yolov5)\n",
      "  Downloading shapely-2.0.6-cp39-cp39-win_amd64.whl.metadata (7.2 kB)\n",
      "Collecting pybboxes==0.1.6 (from sahi>=0.11.10->yolov5)\n",
      "  Downloading pybboxes-0.1.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting terminaltables (from sahi>=0.11.10->yolov5)\n",
      "  Downloading terminaltables-3.1.10-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting click (from sahi>=0.11.10->yolov5)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting numpy>=1.18.5 (from yolov5)\n",
      "  Using cached numpy-1.26.4-cp39-cp39-win_amd64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from tensorboard>=2.4.1->yolov5) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from tensorboard>=2.4.1->yolov5) (1.68.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from tensorboard>=2.4.1->yolov5) (3.7)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from tensorboard>=2.4.1->yolov5) (5.29.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from tensorboard>=2.4.1->yolov5) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from tensorboard>=2.4.1->yolov5) (3.1.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from torch>=1.7.0->yolov5) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from torch>=1.7.0->yolov5) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from torch>=1.7.0->yolov5) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from sympy==1.13.1->torch>=1.7.0->yolov5) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from tqdm>=4.64.0->yolov5) (0.4.6)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from ultralytics>=8.0.100->yolov5) (9.0.0)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from ultralytics>=8.0.100->yolov5) (2.0.13)\n",
      "Requirement already satisfied: termcolor in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from fire->yolov5) (2.5.0)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.23.0->yolov5)\n",
      "  Using cached urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython>=3.1.30->yolov5)\n",
      "  Using cached smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib>=3.3->yolov5) (3.21.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from markdown>=2.6.8->tensorboard>=2.4.1->yolov5) (8.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard>=2.4.1->yolov5) (3.0.2)\n",
      "Downloading yolov5-7.0.14-py37.py38.py39.py310-none-any.whl (953 kB)\n",
      "   ---------------------------------------- 0.0/953.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 953.5/953.5 kB 5.5 MB/s eta 0:00:00\n",
      "Downloading boto3-1.35.77-py3-none-any.whl (139 kB)\n",
      "Using cached GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "Downloading huggingface_hub-0.24.7-py3-none-any.whl (417 kB)\n",
      "Downloading roboflow-1.1.49-py3-none-any.whl (80 kB)\n",
      "Downloading idna-3.7-py3-none-any.whl (66 kB)\n",
      "Downloading opencv_python_headless-4.10.0.84-cp37-abi3-win_amd64.whl (38.8 MB)\n",
      "   ---------------------------------------- 0.0/38.8 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.0/38.8 MB 5.0 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 2.4/38.8 MB 5.8 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 3.7/38.8 MB 6.2 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 5.2/38.8 MB 6.2 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 6.6/38.8 MB 6.3 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 7.9/38.8 MB 6.2 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 9.2/38.8 MB 6.1 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 10.2/38.8 MB 6.0 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 11.0/38.8 MB 5.7 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 11.5/38.8 MB 5.6 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 12.3/38.8 MB 5.2 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 13.1/38.8 MB 5.2 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 13.9/38.8 MB 5.0 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 14.2/38.8 MB 4.9 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 14.4/38.8 MB 4.7 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 14.9/38.8 MB 4.4 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 15.2/38.8 MB 4.3 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 16.0/38.8 MB 4.2 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 17.0/38.8 MB 4.2 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 18.4/38.8 MB 4.3 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 19.1/38.8 MB 4.3 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 20.2/38.8 MB 4.3 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 21.2/38.8 MB 4.3 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 22.3/38.8 MB 4.4 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 23.3/38.8 MB 4.4 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 24.4/38.8 MB 4.4 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 25.4/38.8 MB 4.5 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 26.5/38.8 MB 4.5 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 27.5/38.8 MB 4.5 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 28.8/38.8 MB 4.5 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 29.9/38.8 MB 4.5 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 30.9/38.8 MB 4.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 31.7/38.8 MB 4.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 32.8/38.8 MB 4.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 33.6/38.8 MB 4.5 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 34.3/38.8 MB 4.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 35.4/38.8 MB 4.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 36.4/38.8 MB 4.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 37.5/38.8 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.3/38.8 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.8/38.8 MB 4.5 MB/s eta 0:00:00\n",
      "Downloading sahi-0.11.19-py3-none-any.whl (111 kB)\n",
      "Downloading pybboxes-0.1.6-py3-none-any.whl (24 kB)\n",
      "Using cached numpy-1.26.4-cp39-cp39-win_amd64.whl (15.8 MB)\n",
      "Downloading opencv_python-4.9.0.80-cp37-abi3-win_amd64.whl (38.6 MB)\n",
      "   ---------------------------------------- 0.0/38.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.0/38.6 MB 5.0 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 1.8/38.6 MB 4.8 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 3.1/38.6 MB 5.0 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 3.9/38.6 MB 4.9 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 5.0/38.6 MB 4.9 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 6.3/38.6 MB 4.9 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 7.3/38.6 MB 4.9 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 8.1/38.6 MB 4.8 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 9.4/38.6 MB 4.9 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 10.5/38.6 MB 5.0 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 11.8/38.6 MB 5.0 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 12.8/38.6 MB 5.1 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 13.9/38.6 MB 5.0 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 14.9/38.6 MB 5.0 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 16.0/38.6 MB 5.0 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 17.0/38.6 MB 5.0 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 18.4/38.6 MB 5.1 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 19.4/38.6 MB 5.1 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 20.2/38.6 MB 5.0 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 21.2/38.6 MB 5.0 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 22.3/38.6 MB 5.0 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 23.3/38.6 MB 5.0 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 24.4/38.6 MB 5.0 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 25.4/38.6 MB 5.0 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 27.0/38.6 MB 5.1 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 28.0/38.6 MB 5.1 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 29.4/38.6 MB 5.1 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 30.7/38.6 MB 5.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 32.0/38.6 MB 5.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 33.0/38.6 MB 5.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 34.3/38.6 MB 5.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 35.4/38.6 MB 5.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 36.2/38.6 MB 5.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 37.0/38.6 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  37.7/38.6 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.5/38.6 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.6/38.6 MB 5.0 MB/s eta 0:00:00\n",
      "Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
      "Downloading botocore-1.35.77-py3-none-any.whl (13.2 MB)\n",
      "   ---------------------------------------- 0.0/13.2 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/13.2 MB 4.2 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 2.1/13.2 MB 4.9 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.9/13.2 MB 4.7 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.9/13.2 MB 4.6 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 4.7/13.2 MB 4.7 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.8/13.2 MB 4.6 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 6.6/13.2 MB 4.5 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 7.6/13.2 MB 4.6 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 8.4/13.2 MB 4.5 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 9.4/13.2 MB 4.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 10.7/13.2 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 11.5/13.2 MB 4.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 12.3/13.2 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.2/13.2 MB 4.5 MB/s eta 0:00:00\n",
      "Using cached gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading s3transfer-0.10.4-py3-none-any.whl (83 kB)\n",
      "Downloading shapely-2.0.6-cp39-cp39-win_amd64.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 0.8/1.4 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.4/1.4 MB 4.2 MB/s eta 0:00:00\n",
      "Using cached urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
      "Using cached smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: fire\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114262 sha256=d3d946631ad1ea66be8c40d8f0b8e4b428194b03910e335553539e90aafb82b5\n",
      "  Stored in directory: c:\\users\\abhin\\appdata\\local\\pip\\cache\\wheels\\3b\\ee\\ac\\319a7b7f331f61050d0d54425079b2a883b445be3c7284a4eb\n",
      "Successfully built fire\n",
      "Installing collected packages: filetype, urllib3, terminaltables, smmap, python-dotenv, numpy, jmespath, idna, fire, click, shapely, pybboxes, opencv-python-headless, opencv-python, gitdb, botocore, thop, sahi, s3transfer, requests-toolbelt, huggingface-hub, gitpython, roboflow, boto3, yolov5\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.3\n",
      "    Uninstalling urllib3-2.2.3:\n",
      "      Successfully uninstalled urllib3-2.2.3\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.10\n",
      "    Uninstalling idna-3.10:\n",
      "      Successfully uninstalled idna-3.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\abhin\\anaconda3\\envs\\assignment_env\\Lib\\site-packages\\~umpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\abhin\\anaconda3\\envs\\assignment_env\\Lib\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'c:\\\\Users\\\\abhin\\\\anaconda3\\\\envs\\\\assignment_env\\\\Lib\\\\site-packages\\\\cv2\\\\cv2.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install yolov5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\abhin/.cache\\torch\\hub\\ultralytics_yolov5_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['gitpython>=3.1.30'] not found, attempting AutoUpdate...\n",
      "Collecting gitpython>=3.1.30\n",
      "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython>=3.1.30)\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.30) (5.0.1)\n",
      "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Installing collected packages: gitdb, gitpython\n",
      "Successfully installed gitdb-4.0.11 gitpython-3.1.43\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success  4.5s, installed 1 package: ['gitpython>=3.1.30']\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m  \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  2024-12-10 Python-3.9.20 torch-2.5.1+cpu CPU\n",
      "\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n",
      "100%|██████████| 14.1M/14.1M [00:03<00:00, 4.24MB/s]\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n",
      "C:\\Users\\abhin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved the image with bounding boxes to output_image_with_bboxes.jpg\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load the pre-trained YOLOv5 model (small version in this example)\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Step 2: Load the image\n",
    "image_path = 'images (1).jpeg'  # Replace with your image path\n",
    "img = Image.open(image_path)\n",
    "\n",
    "# Step 3: Perform inference on the image using the YOLO model\n",
    "results = model(img)\n",
    "\n",
    "# Step 4: Get the bounding boxes, labels, and confidence scores\n",
    "boxes = results.xywh[0][:, :-2]  # Bounding boxes (x, y, width, height)\n",
    "labels = results.xywh[0][:, -2].int()  # Labels of detected objects\n",
    "scores = results.xywh[0][:, -1]  # Confidence scores\n",
    "\n",
    "# Step 5: Convert the image to numpy array for OpenCV compatibility\n",
    "img_cv = np.array(img)\n",
    "\n",
    "# Step 6: Draw the bounding boxes on the image\n",
    "draw = ImageDraw.Draw(img)\n",
    "for box, label, score in zip(boxes, labels, scores):\n",
    "    x1, y1, w, h = box\n",
    "    x1, y1, w, h = int(x1 - w / 2), int(y1 - h / 2), int(w), int(h)  # Convert to top-left corner and width/height\n",
    "\n",
    "    # Draw bounding box\n",
    "    draw.rectangle([x1, y1, x1 + w, y1 + h], outline=\"red\", width=3)\n",
    "\n",
    "    # Add label and confidence score\n",
    "    label_text = f\"Label: {label.item()} Score: {score:.2f}\"\n",
    "    draw.text((x1, y1 - 10), label_text, fill=\"red\")\n",
    "\n",
    "# Step 7: Save the resulting image with bounding boxes\n",
    "output_image_path = 'output_image_with_bboxes.jpg'  # Path to save the image\n",
    "img.save(output_image_path)\n",
    "\n",
    "# Optionally, display the image\n",
    "img.show()\n",
    "\n",
    "print(f\"Saved the image with bounding boxes to {output_image_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
