{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. What is NLTK?**\n",
    "\n",
    "NLTK (Natural Language Toolkit) is a Python library used for working with human language data. It provides tools for text processing, including tokenization, parsing, classification, stemming, tagging, and more. It is widely used for educational purposes and prototyping NLP tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. What is SpaCy and how does it differ from NLTK?**\n",
    "\n",
    "SpaCy is a modern, industrial-strength NLP library that focuses on performance, ease of use, and providing pre-trained models. It is faster than NLTK and provides more advanced features like named entity recognition (NER), dependency parsing, and word vectors. Unlike NLTK, SpaCy is primarily aimed at production systems rather than educational use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. What is the purpose of TextBlob in NLP?**\n",
    "\n",
    "TextBlob is a Python library for processing textual data. It provides easy-to-use APIs for common NLP tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, and translation. It's designed to be simpler and more intuitive than libraries like NLTK and SpaCy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. What is Stanford NLP?**\n",
    "\n",
    "Stanford NLP is a suite of NLP tools developed by the Stanford NLP Group. It offers pre-trained models and tools for text analysis tasks such as part-of-speech tagging, named entity recognition (NER), dependency parsing, and more. It is known for its high accuracy and is often used in academic research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Explain what Recurrent Neural Networks (RNN) are.**  \n",
    "\n",
    "RNNs are a class of neural networks designed for sequence data, such as time-series data or text. Unlike traditional neural networks, RNNs have connections that allow information to be passed from one step to the next, making them suitable for tasks like speech recognition, language modeling, and text generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. What is the main advantage of using LSTM over RNN?**  \n",
    "\n",
    "LSTM (Long Short-Term Memory) networks are a type of RNN that overcome the vanishing gradient problem of traditional RNNs. LSTMs can capture long-term dependencies in sequence data, which makes them more effective than basic RNNs for tasks like machine translation or speech recognition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. What are Bi-directional LSTMs, and how do they differ from standard LSTMs?**  \n",
    "\n",
    "Bi-directional LSTMs process data in both forward and backward directions, allowing the model to consider both past and future context in sequence data. Standard LSTMs only process data in one direction (usually from past to future). Bi-directional LSTMs improve performance in tasks like sentiment analysis and machine translation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. What is the purpose of a Stacked LSTM?**  \n",
    "\n",
    "A Stacked LSTM refers to multiple LSTM layers stacked on top of each other. This architecture allows the model to capture more complex patterns and hierarchical dependencies in sequential data. It is commonly used to improve the performance of deep learning models for sequence-to-sequence tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. How does a GRU (Gated Recurrent Unit) differ from an LSTM?**  \n",
    "\n",
    "GRUs are a simpler variant of LSTMs. They use fewer gates (two instead of three) to control the flow of information, making them computationally more efficient. While LSTMs are better at capturing long-term dependencies, GRUs perform similarly in many tasks and are faster to train.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10. What are the key features of NLTK's tokenization process?**  \n",
    "\n",
    "NLTK provides several tokenization methods, including word and sentence tokenizers. It can handle complex tokenization tasks such as splitting contractions, punctuation, and other non-word elements. NLTK's tokenization tools are highly customizable and can work with different languages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**11. How do you perform named entity recognition (NER) using SpaCy?**  \n",
    "\n",
    "In SpaCy, NER is performed by processing a text with a pre-trained model. After processing, the named entities can be accessed through the `ents` attribute of the processed text object. Here’s an example:\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(\"Apple is planning to open a new store in New York.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**12. What is Word2Vec and how does it represent words?**  \n",
    "\n",
    "Word2Vec is a technique for representing words as continuous vector spaces. It uses a shallow neural network to map each word in a vocabulary to a vector. These vectors capture semantic relationships between words, allowing words with similar meanings to have similar representations in the vector space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**13. Explain the difference between Bag of Words (BoW) and Word2Vec.**  \n",
    "\n",
    "BoW represents text as a collection of words without considering the order or context in which they appear. Each word is treated as an independent feature. In contrast, Word2Vec captures semantic relationships and word contexts by representing words as dense vectors in a continuous space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**14. How does TextBlob handle sentiment analysis?**  \n",
    "\n",
    "TextBlob provides a built-in method for performing sentiment analysis. It assigns a polarity score (ranging from -1 for negative to 1 for positive) and a subjectivity score (ranging from 0 for objective to 1 for subjective) to text. The polarity score is used to determine whether the sentiment of the text is positive, negative, or neutral.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**15. How would you implement text preprocessing using NLTK?**  \n",
    "\n",
    "Text preprocessing using NLTK involves steps like tokenization, removing stopwords, stemming/lemmatization, and lowercasing. For example:\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = \"This is an example sentence.\"\n",
    "tokens = word_tokenize(text)\n",
    "tokens = [word.lower() for word in tokens if word not in stopwords.words('english')]\n",
    "stemmed_tokens = [PorterStemmer().stem(word) for word in tokens]\n",
    "print(stemmed_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**16. How do you train a custom NER model using SpaCy?**  \n",
    "\n",
    "Training a custom NER model in SpaCy involves creating an annotated dataset with entities and labels, and then using SpaCy's `train()` function to train the model. You must first create a blank model and then update it with new examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**17. What is the role of the attention mechanism in LSTMs and GRUs?**  \n",
    "\n",
    "The attention mechanism allows the model to focus on specific parts of the input sequence when making predictions. In sequence-to-sequence models, it helps the model decide which words or phrases to pay attention to when generating output, improving the quality of tasks like machine translation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**18. What is the difference between tokenization and lemmatization in NLP?**  \n",
    "\n",
    "Tokenization is the process of breaking a text into individual units, such as words or sentences. Lemmatization, on the other hand, reduces words to their base form (e.g., \"running\" becomes \"run\"). While tokenization splits text, lemmatization normalizes words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**19. How do you perform text normalization in NLP?**  \n",
    "\n",
    "Text normalization involves converting text to a consistent format by removing noise, converting all characters to lowercase, removing special characters, and lemmatizing words. This helps to ensure that variations of a word are treated as the same word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**20. What is the purpose of frequency distribution in NLP?**  \n",
    "\n",
    "Frequency distribution in NLP is used to track how often words or tokens appear in a text corpus. It helps identify the most common words and can provide insight into the thematic focus of a text or document.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**21. What are co-occurrence vectors in NLP?**  \n",
    "\n",
    "Co-occurrence vectors represent the context of a word by counting how often it appears with other words in a given window of text. These vectors can capture semantic relationships between words based on their proximity to each other in the text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**22. How is Word2Vec used to find the relationship between words?**  \n",
    "\n",
    "Word2Vec learns relationships between words by training on large corpora and capturing context. The learned vectors can be used to calculate relationships between words, such as \"king\" - \"man\" + \"woman\" = \"queen\", by performing vector arithmetic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**23. How does a Bi-LSTM improve NLP tasks compared to a regular LSTM?**  \n",
    "\n",
    "A Bi-LSTM processes text in both forward and backward directions, allowing the model to consider both past and future context. This can improve performance in tasks that require understanding the full context of a sentence, such as sentiment analysis or named entity recognition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**24. What is the difference between a GRU and an LSTM in terms of gate structures?**  \n",
    "\n",
    "LSTM networks use three gates (input, forget, and output gates) to regulate the flow of information. GRUs use two gates (update and reset gates), making them simpler and computationally more efficient than LSTMs, while still capturing long-term dependencies in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**25. How does Stanford NLP’s dependency parsing work?**  \n",
    "\n",
    "Dependency parsing in Stanford NLP analyzes the grammatical structure of a sentence, identifying relationships between words. It creates a tree structure where words are connected based on their syntactic dependencies, helping to understand the syntactic structure of sentences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**26. How does tokenization affect downstream NLP tasks?**  \n",
    "\n",
    "Tokenization affects downstream NLP tasks by determining how the text is broken down. Incorrect tokenization can lead to misinterpretation of text, while well-executed tokenization ensures that each word or unit is properly recognized for tasks like named entity recognition, sentiment analysis, and machine translation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**27. What are some common applications of NLP?**  \n",
    "\n",
    "Common applications of NLP include machine translation, sentiment analysis, chatbots, text summarization, named entity recognition (NER), information retrieval, and speech recognition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**28. What are stopwords and why are they removed in NLP?**  \n",
    "\n",
    "Stopwords are common words like \"the\", \"is\", and \"in\" that do not add significant meaning to the text. They are often removed during text preprocessing to reduce dimensionality and focus on the most important words in the analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**29. How can you implement word embeddings using Word2Vec in Python?**  \n",
    "\n",
    "You can use the `gensim` library to implement Word2Vec in Python. Here's an example:\n",
    "\n",
    "```python\n",
    "from gensim.models import Word2Vec\n",
    "sentences = [[\"hello\", \"world\"], [\"goodbye\", \"world\"]]\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "vector = model.wv['hello']\n",
    "print(vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**30. How does SpaCy handle lemmatization?**  \n",
    "\n",
    "SpaCy's lemmatization process reduces words to their base or dictionary form. It uses pre-trained models to identify the root form of a word based on context. For example, \"running\" would be lemmatized to \"run\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**31. What is the significance of RNNs in NLP tasks?**  \n",
    "\n",
    "RNNs are significant in NLP because they can process sequences of data, such as text, where the order of words matters. They capture dependencies between words in a sequence and are widely used in tasks like machine translation, language modeling, and speech recognition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**32. How does text classification work using machine learning?**  \n",
    "\n",
    "Text classification involves assigning predefined labels to text based on its content. The process includes preprocessing the text, converting it into numerical features (e.g., using TF-IDF or word embeddings), training a machine learning model (e.g., logistic regression, SVM, or neural networks), and using the model to predict labels for new text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**33. What is the importance of word frequency in NLP?**  \n",
    "\n",
    "Word frequency provides insights into the relevance of a word within a corpus. Words with higher frequency often carry more weight and are more likely to be important in the context of text classification, topic modeling, and sentiment analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. How do you perform word tokenization using NLTK and plot a word frequency distribution?\n",
    "\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Sample text\n",
    "text = \"This is an example sentence. This sentence is for word tokenization.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text.lower())\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "\n",
    "# Create a frequency distribution\n",
    "fdist = FreqDist(tokens)\n",
    "\n",
    "# Plot the frequency distribution\n",
    "fdist.plot(30, cumulative=False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load SpaCy's small English model\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men_core_web_sm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Process a sentence\u001b[39;00m\n\u001b[0;32m      9\u001b[0m sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpaCy is an NLP library.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages\\spacy\\__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages\\spacy\\util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "# 2. How do you use SpaCy for dependency parsing of a sentence?\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Load SpaCy's small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process a sentence\n",
    "sentence = \"SpaCy is an NLP library.\"\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Dependency parsing\n",
    "for token in doc:\n",
    "    print(f\"{token.text} -> {token.dep_} -> {token.head.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: positive\n"
     ]
    }
   ],
   "source": [
    "# 3. How do you use TextBlob for performing text classification based on polarity?\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Example text\n",
    "text = \"I love this place, it's amazing!\"\n",
    "\n",
    "# Create a TextBlob object\n",
    "blob = TextBlob(text)\n",
    "\n",
    "# Get the polarity score\n",
    "polarity = blob.sentiment.polarity\n",
    "classification = \"positive\" if polarity > 0 else \"negative\" if polarity < 0 else \"neutral\"\n",
    "\n",
    "print(f\"Sentiment: {classification}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load SpaCy's small English model\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men_core_web_sm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Sample text\u001b[39;00m\n\u001b[0;32m      9\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApple is planning to open a new store in New York.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages\\spacy\\__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages\\spacy\\util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "# 4. How do you extract named entities from a text using SpaCy?\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Load SpaCy's small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text\n",
    "text = \"Apple is planning to open a new store in New York.\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract named entities\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text} -> {ent.label_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.5        0.\n",
      "  0.5        0.         0.5        0.         0.         0.5       ]\n",
      " [0.         0.54935123 0.         0.         0.41779577 0.\n",
      "  0.41779577 0.         0.41779577 0.         0.         0.41779577]\n",
      " [0.31622777 0.         0.31622777 0.31622777 0.         0.31622777\n",
      "  0.         0.31622777 0.         0.31622777 0.63245553 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# 5. How can you calculate TF-IDF scores for a given text using Scikit-learn?\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Example corpus\n",
    "corpus = [\n",
    "    \"This is a sample document.\",\n",
    "    \"This document is another sample.\",\n",
    "    \"Text mining and text classification are important tasks.\"\n",
    "]\n",
    "\n",
    "# Create the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the corpus\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert the matrix to a dense array and display\n",
    "print(tfidf_matrix.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy: 0.7188888888888889\n"
     ]
    }
   ],
   "source": [
    "# 6. How do you create a custom text classifier using NLTK's Naive Bayes classifier?\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "# Load movie review data (positive and negative reviews)\n",
    "positive_reviews = movie_reviews.fileids('pos')\n",
    "negative_reviews = movie_reviews.fileids('neg')\n",
    "\n",
    "# Feature extraction function\n",
    "def extract_features(words):\n",
    "    return {word: True for word in words}\n",
    "\n",
    "# Prepare the dataset\n",
    "positive_features = [(extract_features(movie_reviews.words(fileid)), 'pos') for fileid in positive_reviews]\n",
    "negative_features = [(extract_features(movie_reviews.words(fileid)), 'neg') for fileid in negative_reviews]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_set = positive_features[:100] + negative_features[:100]\n",
    "test_set = positive_features[100:] + negative_features[100:]\n",
    "\n",
    "# Train the Naive Bayes classifier\n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# Test the classifier\n",
    "print(f\"Classifier accuracy: {nltk.classify.accuracy(classifier, test_set)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.distilbert.modeling_tf_distilbert because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages\\transformers\\activations_tf.py:22\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tf_keras'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages\\transformers\\utils\\import_utils.py:1793\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1792\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1793\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\importlib\\__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1030\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:850\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:228\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_tf_distilbert.py:27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations_tf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_tf_activation\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_tf_outputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     29\u001b[0m     TFBaseModelOutput,\n\u001b[0;32m     30\u001b[0m     TFMaskedLMOutput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     TFTokenClassifierOutput,\n\u001b[0;32m     35\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages\\transformers\\activations_tf.py:27\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m parse(keras\u001b[38;5;241m.\u001b[39m__version__)\u001b[38;5;241m.\u001b[39mmajor \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     28\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour currently installed version of Keras is Keras 3, but this is not yet supported in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     29\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformers. Please install the backwards-compatible tf-keras package with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     30\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install tf-keras`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     31\u001b[0m         )\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_gelu\u001b[39m(x):\n",
      "\u001b[1;31mValueError\u001b[0m: Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load a pre-trained sentiment-analysis model\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m classifier \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msentiment-analysis\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Example text\u001b[39;00m\n\u001b[0;32m      9\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI love machine learning!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages\\transformers\\pipelines\\__init__.py:940\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    939\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m--> 940\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m infer_framework_load_model(\n\u001b[0;32m    941\u001b[0m         model,\n\u001b[0;32m    942\u001b[0m         model_classes\u001b[38;5;241m=\u001b[39mmodel_classes,\n\u001b[0;32m    943\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m    944\u001b[0m         framework\u001b[38;5;241m=\u001b[39mframework,\n\u001b[0;32m    945\u001b[0m         task\u001b[38;5;241m=\u001b[39mtask,\n\u001b[0;32m    946\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[0;32m    947\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m    948\u001b[0m     )\n\u001b[0;32m    950\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    951\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[1;32mc:\\Users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages\\transformers\\pipelines\\base.py:264\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    262\u001b[0m         classes\u001b[38;5;241m.\u001b[39mappend(_class)\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m look_tf:\n\u001b[1;32m--> 264\u001b[0m     _class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtransformers_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTF\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43marchitecture\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    266\u001b[0m         classes\u001b[38;5;241m.\u001b[39mappend(_class)\n",
      "File \u001b[1;32mc:\\Users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages\\transformers\\utils\\import_utils.py:1782\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1781\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[1;32m-> 1782\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n\u001b[0;32m   1784\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n",
      "File \u001b[1;32mc:\\Users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages\\transformers\\utils\\import_utils.py:1781\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1779\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1781\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1782\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n",
      "File \u001b[1;32mc:\\Users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages\\transformers\\utils\\import_utils.py:1795\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1793\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1795\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1796\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1797\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1798\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.models.distilbert.modeling_tf_distilbert because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`."
     ]
    }
   ],
   "source": [
    "# 7. How do you use a pre-trained model from Hugging Face for text classification?\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load a pre-trained sentiment-analysis model\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Example text\n",
    "text = \"I love machine learning!\"\n",
    "\n",
    "# Classify the text\n",
    "result = classifier(text)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "c:\\Users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\abhin\\.cache\\huggingface\\hub\\models--sshleifer--distilbart-cnn-12-6. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.bart.modeling_tf_bart because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages\\transformers\\activations_tf.py:22\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tf_keras'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages\\transformers\\utils\\import_utils.py:1793\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1792\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1793\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\importlib\\__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1030\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:850\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:228\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages\\transformers\\models\\bart\\modeling_tf_bart.py:25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations_tf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_tf_activation\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_tf_outputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     27\u001b[0m     TFBaseModelOutput,\n\u001b[0;32m     28\u001b[0m     TFBaseModelOutputWithPastAndCrossAttentions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m     TFSeq2SeqSequenceClassifierOutput,\n\u001b[0;32m     32\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages\\transformers\\activations_tf.py:27\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m parse(keras\u001b[38;5;241m.\u001b[39m__version__)\u001b[38;5;241m.\u001b[39mmajor \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     28\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour currently installed version of Keras is Keras 3, but this is not yet supported in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     29\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformers. Please install the backwards-compatible tf-keras package with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     30\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install tf-keras`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     31\u001b[0m         )\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_gelu\u001b[39m(x):\n",
      "\u001b[1;31mValueError\u001b[0m: Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load the pre-trained summarization model\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m summarizer \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msummarization\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Example long text\u001b[39;00m\n\u001b[0;32m      9\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124mHugging Face is a company specializing in Natural Language Processing (NLP). It has become one of the leading organizations \u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124min the AI community, developing various open-source libraries, including Transformers, which provide access to pre-trained \u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124mmodels that simplify the process of using state-of-the-art machine learning techniques in NLP.\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages\\transformers\\pipelines\\__init__.py:940\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    939\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m--> 940\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m infer_framework_load_model(\n\u001b[0;32m    941\u001b[0m         model,\n\u001b[0;32m    942\u001b[0m         model_classes\u001b[38;5;241m=\u001b[39mmodel_classes,\n\u001b[0;32m    943\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m    944\u001b[0m         framework\u001b[38;5;241m=\u001b[39mframework,\n\u001b[0;32m    945\u001b[0m         task\u001b[38;5;241m=\u001b[39mtask,\n\u001b[0;32m    946\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[0;32m    947\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m    948\u001b[0m     )\n\u001b[0;32m    950\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    951\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[1;32mc:\\Users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages\\transformers\\pipelines\\base.py:264\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    262\u001b[0m         classes\u001b[38;5;241m.\u001b[39mappend(_class)\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m look_tf:\n\u001b[1;32m--> 264\u001b[0m     _class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtransformers_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTF\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43marchitecture\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    266\u001b[0m         classes\u001b[38;5;241m.\u001b[39mappend(_class)\n",
      "File \u001b[1;32mc:\\Users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages\\transformers\\utils\\import_utils.py:1782\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1781\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[1;32m-> 1782\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n\u001b[0;32m   1784\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n",
      "File \u001b[1;32mc:\\Users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages\\transformers\\utils\\import_utils.py:1781\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1779\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1781\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1782\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n",
      "File \u001b[1;32mc:\\Users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages\\transformers\\utils\\import_utils.py:1795\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1793\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1795\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1796\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1797\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1798\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.models.bart.modeling_tf_bart because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`."
     ]
    }
   ],
   "source": [
    "# 8. How do you perform text summarization using Hugging Face transformers?\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the pre-trained summarization model\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "# Example long text\n",
    "text = \"\"\"\n",
    "Hugging Face is a company specializing in Natural Language Processing (NLP). It has become one of the leading organizations \n",
    "in the AI community, developing various open-source libraries, including Transformers, which provide access to pre-trained \n",
    "models that simplify the process of using state-of-the-art machine learning techniques in NLP.\n",
    "\"\"\"\n",
    "\n",
    "# Perform the summarization\n",
    "summary = summarizer(text, max_length=50, min_length=25, do_sample=False)\n",
    "print(summary[0]['summary_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_7\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)          │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_6 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn (\u001b[38;5;33mSimpleRNN\u001b[0m)          │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 9. How can you create a simple RNN for text classification using Keras?\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Sample data\n",
    "texts = [\"I love machine learning\", \"NLP is amazing\", \"Deep learning is powerful\"]\n",
    "labels = [1, 1, 1]  # All positive sentiment\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "X = pad_sequences(sequences)\n",
    "\n",
    "# Build the RNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=1000, output_dim=64, input_length=X.shape[1]))\n",
    "model.add(SimpleRNN(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_6\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_5 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_3 (\u001b[38;5;33mBidirectional\u001b[0m) │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 10. How do you train a Bidirectional LSTM for text classification?\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Bidirectional, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Sample data\n",
    "texts = [\"I love machine learning\", \"NLP is amazing\", \"Deep learning is powerful\"]\n",
    "labels = [1, 1, 1]  # All positive sentiment\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "X = pad_sequences(sequences)\n",
    "\n",
    "# Build the Bidirectional LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=1000, output_dim=64, input_length=X.shape[1]))\n",
    "model.add(Bidirectional(LSTM(64, activation='relu')))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_1 (\u001b[38;5;33mGRU\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 11. How do you implement GRU (Gated Recurrent Unit) for text classification?\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, GRU, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Sample data\n",
    "texts = [\"I love machine learning\", \"NLP is amazing\", \"Deep learning is powerful\"]\n",
    "labels = [1, 1, 1]  # All positive sentiment\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "X = pad_sequences(sequences)\n",
    "\n",
    "# Build the GRU model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=1000, output_dim=64, input_length=X.shape[1]))\n",
    "model.add(GRU(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">66,560</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m66,560\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation (\u001b[38;5;33mActivation\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">66,689</span> (260.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m66,689\u001b[0m (260.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">66,689</span> (260.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m66,689\u001b[0m (260.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 12. How do you implement a text generation model using LSTM with Keras?\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Activation\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Sample data\n",
    "text = \"This is an example text generation task using LSTM.\"\n",
    "\n",
    "# Create a model for text generation\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(10, 1)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy')\n",
    "\n",
    "# Example to train and generate text (simplified)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 13. How do you implement a simple Bi-directional GRU for sequence labeling?\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Bidirectional, GRU, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Sample data\n",
    "texts = [\"I love machine learning\", \"NLP is amazing\", \"Deep learning is powerful\"]\n",
    "labels = [1, 1, 1]  # All positive sentiment\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "X = pad_sequences(sequences)\n",
    "\n",
    "# Build the Bi-directional GRU model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=1000, output_dim=64, input_length=X.shape[1]))\n",
    "model.add(Bidirectional(GRU(64, activation='relu')))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
