{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **THEORETICAL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **What is the primary goal of Natural Language Processing (NLP)?**\n",
    "\n",
    "NLP aims to enable machines to understand, interpret, and respond to human language in a meaningful way, facilitating tasks such as translation, sentiment analysis, and text summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **What does \"tokenization\" refer to in text processing?**\n",
    "\n",
    "Tokenization is the process of splitting text into smaller units, like words, phrases, or sentences, which are called tokens. These tokens are used as inputs for NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **What is the difference between lemmatization and stemming?**\n",
    "\n",
    "Stemming reduces words to their root form by removing suffixes (e.g., \"running\" → \"run\"), often leading to non-standard forms. Lemmatization, however, uses a vocabulary to return the base or dictionary form of a word (e.g., \"better\" → \"good\").\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **What is the role of regular expressions (regex) in text processing?**\n",
    "\n",
    "Regex is used to search, match, and manipulate text by defining patterns, making tasks like data cleaning, extraction, and formatting efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **What is Word2Vec, and how does it represent words in a vector space?**\n",
    "\n",
    "Word2Vec is a neural network model that represents words as dense vectors in a high-dimensional space. It captures semantic relationships, such as synonyms being close in the vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **How does frequency distribution help in text analysis?**\n",
    "\n",
    "Frequency distribution identifies the occurrence of words or tokens in a dataset, helping to determine the importance or relevance of terms and their patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. **Why is text normalization important in NLP?**\n",
    "\n",
    "Text normalization ensures consistency by converting text into a standard format, reducing noise. Techniques include lowercasing, removing punctuation, and stemming/lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. **What is the difference between sentence tokenization and word tokenization?**\n",
    "\n",
    "Sentence tokenization divides text into sentences, while word tokenization splits sentences into individual words or tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. **What are co-occurrence vectors in NLP?**\n",
    "\n",
    "Co-occurrence vectors represent the frequency with which words appear together within a specified context, revealing relationships and meanings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. **What is the significance of lemmatization in improving NLP tasks?**\n",
    "\n",
    "Lemmatization improves accuracy by reducing words to their meaningful base forms, aiding in semantic understanding for tasks like search or chatbots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. **What is the primary use of word embeddings in NLP?**\n",
    "\n",
    "Word embeddings represent words in a continuous vector space, capturing semantic meanings and improving performance in tasks like sentiment analysis and translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. **What is an annotator in NLP?**\n",
    "\n",
    "An annotator labels text data with relevant information, such as parts of speech, named entities, or syntactic dependencies, crucial for supervised learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. **What are the key steps in text processing before applying machine learning models?**\n",
    "\n",
    "Key steps include text cleaning, tokenization, normalization, lemmatization/stemming, stop-word removal, and feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. **What is the history of NLP, and how has it evolved?**\n",
    "\n",
    "NLP evolved from rule-based methods in the 1950s to statistical models in the 1990s, and now to neural networks and deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. **Why is sentence processing important in NLP?**\n",
    "\n",
    "Sentence processing preserves grammatical and semantic context, improving the understanding of text for downstream tasks like summarization or translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. **How do word embeddings improve the understanding of language semantics in NLP?**\n",
    "\n",
    "Word embeddings capture relationships between words by mapping them to vectors in a continuous space, reflecting semantic similarity and context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17. **How does the frequency distribution of words help in text classification?**\n",
    "\n",
    "It helps identify significant features, such as commonly occurring words in specific categories, aiding model training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18. **What are the advantages of using regex in text cleaning?**\n",
    "\n",
    "Regex provides a flexible and efficient way to clean text by defining patterns to identify unwanted elements like noise or specific data formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19. **What is the difference between Word2Vec and Doc2Vec?**\n",
    "\n",
    "Word2Vec represents individual words, while Doc2Vec represents entire documents, capturing the context of a text beyond word-level semantics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20. **Why is understanding text normalization important in NLP?**\n",
    "\n",
    "It ensures consistency and improves model performance by handling variations like casing, misspellings, or irrelevant formatting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21. **How does word count help in text analysis?**\n",
    "\n",
    "Word count reveals text length, frequency patterns, and highlights key topics, assisting in summarization and keyword extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22. **How does lemmatization help in NLP tasks like search engines and chatbots?**\n",
    "\n",
    "Lemmatization improves query matching and response accuracy by standardizing words to their meaningful base forms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "23. **What is the purpose of using Doc2Vec in text processing?**\n",
    "\n",
    "Doc2Vec creates fixed-length representations for documents, making it useful for tasks like document similarity or classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24. **What is text normalization, and what are the common techniques used in it?**\n",
    "\n",
    "Text normalization converts text into a standard format. Techniques include lowercasing, removing stop words, stemming, and lemmatization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25. **Why is word tokenization important in NLP?**\n",
    "\n",
    "It breaks text into manageable units, allowing models to analyze and process words individually for meaningful predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "26. **How does sentence tokenization differ from word tokenization in NLP?**\n",
    "\n",
    "Sentence tokenization splits text into sentences, while word tokenization divides sentences into words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "27. **What is the primary purpose of text processing in NLP?**\n",
    "\n",
    "It prepares raw text for analysis by cleaning, structuring, and extracting relevant features for better model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "28. **What are the key challenges in NLP?**\n",
    "\n",
    "Challenges include ambiguity, context understanding, handling low-resource languages, and processing noisy or unstructured data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "29. **How do co-occurrence vectors represent relationships between words?**\n",
    "\n",
    "Co-occurrence vectors show how often words appear together in a context, revealing their associations and meanings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "30. **What is the impact of word embeddings on NLP tasks?**\n",
    "\n",
    "Word embeddings enhance performance by capturing semantic and syntactic relationships, enabling better context understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "31. **What is the purpose of using lemmatization in text preprocessing?**\n",
    "\n",
    "Lemmatization standardizes words to their base forms, improving text consistency and model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PRACTICAL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Collecting joblib (from nltk)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Using cached regex-2024.11.6-cp39-cp39-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached regex-2024.11.6-cp39-cp39-win_amd64.whl (274 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Installing collected packages: regex, joblib, nltk\n",
      "Successfully installed joblib-1.4.2 nltk-3.9.1 regex-2024.11.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\abhin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '!', 'How', 'are', 'you', 'doing', 'today', '?']\n"
     ]
    }
   ],
   "source": [
    "#1. How can you perform word tokenization using NLTK\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download the necessary data for tokenization\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Input text\n",
    "text = \"Hello, world! How are you doing today?\"\n",
    "\n",
    "# Perform word tokenization\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Display the tokens\n",
    "print(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello, world!', 'How are you doing today?']\n"
     ]
    }
   ],
   "source": [
    "#2. How can you perform sentence tokenization using NLTK:\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "tokens=sent_tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "['Hello', ',', 'world', '!', 'How', 'are', 'you', 'doing', 'today', '?']\n",
      "['Hello', ',', 'world', '!', 'How', 'you', 'today', '?']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\abhin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#3. How can you remove stopwords from a sentence?\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "nltk.download('stopwords')\n",
    "stopword=stopwords.words('english')\n",
    "print(stopwords.words('english'))\n",
    "\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Display the tokens\n",
    "print(tokens)\n",
    "for char in tokens:\n",
    "    if char in stopword:\n",
    "        tokens.remove(char)\n",
    "\n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      ",\n",
      "world\n",
      "!\n",
      "how\n",
      "you\n",
      "today\n",
      "?\n"
     ]
    }
   ],
   "source": [
    "#4. How can you perform stemming on a word?\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for x in tokens:\n",
    "    print(ps.stem(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n",
      "better : good\n"
     ]
    }
   ],
   "source": [
    "#5. How can you perform lemmatization on a word?\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    " \n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
    " \n",
    "# a denotes adjective in \"pos\"\n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos=\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', 'how', 'are', 'you', 'doing', 'today']\n"
     ]
    }
   ],
   "source": [
    "#6.  How can you normalize a text by converting it to lowercase and removing punctuation\n",
    "import re\n",
    "text = \"Hello, world! How are you doing today?\"\n",
    "normalize_text=\"\"\n",
    "for char in text:\n",
    "    normalize_text+= char.lower()\n",
    "\n",
    "tokens = word_tokenize(normalize_text)\n",
    "\n",
    "cleaned_tokens = [re.sub(r'[^\\w\\s]', '', token) for token in tokens if re.sub(r'[^\\w\\s]', '', token)]\n",
    "print(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['fascinating', 'language', 'learning', 'love', 'natural', 'new', 'nlp', 'processing', 'things']\n",
      "\n",
      "Co-occurrence Matrix:\n",
      "[[0 1 0 0 0 0 0 1 0]\n",
      " [1 0 0 1 1 0 0 2 0]\n",
      " [0 0 0 1 0 1 0 0 1]\n",
      " [0 1 1 0 1 1 0 0 0]\n",
      " [0 1 0 1 0 0 0 1 0]\n",
      " [0 0 1 1 0 0 1 0 1]\n",
      " [0 0 0 0 0 1 0 0 1]\n",
      " [1 2 0 0 1 0 0 0 0]\n",
      " [0 0 1 0 0 1 1 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\abhin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\abhin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#7. How can you create a co-occurrence matrix for words in a corpus?\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Input text corpus\n",
    "corpus = [\n",
    "    \"I love natural language processing.\",\n",
    "    \"Language processing is fascinating.\",\n",
    "    \"I love learning new things about NLP.\"\n",
    "]\n",
    "\n",
    "# Parameters\n",
    "window_size = 2  # Context window size\n",
    "\n",
    "# Preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "corpus = [word_tokenize(doc.lower()) for doc in corpus]\n",
    "corpus = [[word for word in doc if word not in stop_words and word not in string.punctuation] for doc in corpus]\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = sorted(set(word for doc in corpus for word in doc))\n",
    "word_to_index = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "# Initialize co-occurrence matrix\n",
    "co_matrix = np.zeros((len(vocab), len(vocab)), dtype=int)\n",
    "\n",
    "# Populate the matrix\n",
    "for doc in corpus:\n",
    "    for i, word in enumerate(doc):\n",
    "        word_idx = word_to_index[word]\n",
    "        # Define the context window\n",
    "        start = max(i - window_size, 0)\n",
    "        end = min(i + window_size + 1, len(doc))\n",
    "        for j in range(start, end):\n",
    "            if i != j:  # Exclude the word itself\n",
    "                co_matrix[word_idx, word_to_index[doc[j]]] += 1\n",
    "\n",
    "# Display the matrix\n",
    "print(\"Vocabulary:\", vocab)\n",
    "print(\"\\nCo-occurrence Matrix:\")\n",
    "print(co_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abc@gmail.com', 'abc@yahoo.com']\n"
     ]
    }
   ],
   "source": [
    "#8. How can you apply a regular expression to extract all email addresses from a text?\n",
    "\n",
    "import re\n",
    "\n",
    "text = \"abc@gmail.com, abc@yahoo.com,hii\"\n",
    "\n",
    "# Regular expression to match email addresses\n",
    "pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
    "\n",
    "# Use re.findall to extract all matching email addresses\n",
    "emails = re.findall(pattern, text)\n",
    "\n",
    "print(emails)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp39-cp39-win_amd64.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: wrapt in c:\\users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.0)\n",
      "Downloading gensim-4.3.3-cp39-cp39-win_amd64.whl (24.0 MB)\n",
      "   ---------------------------------------- 0.0/24.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.0/24.0 MB 7.2 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 2.6/24.0 MB 7.9 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 4.2/24.0 MB 7.9 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 5.5/24.0 MB 7.8 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 7.3/24.0 MB 7.7 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 8.7/24.0 MB 7.4 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 9.4/24.0 MB 6.8 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 10.0/24.0 MB 6.4 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 10.2/24.0 MB 5.9 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 10.5/24.0 MB 5.4 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 10.7/24.0 MB 5.0 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 11.3/24.0 MB 4.6 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 11.5/24.0 MB 4.5 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 12.1/24.0 MB 4.3 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 12.6/24.0 MB 4.1 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 13.1/24.0 MB 4.0 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 13.9/24.0 MB 4.0 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 14.4/24.0 MB 3.9 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.2/24.0 MB 3.9 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 16.0/24.0 MB 3.9 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 16.8/24.0 MB 3.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 17.6/24.0 MB 3.9 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 18.4/24.0 MB 3.9 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 19.1/24.0 MB 3.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.2/24.0 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 20.7/24.0 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 21.8/24.0 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.5/24.0 MB 3.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.3/24.0 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.0/24.0 MB 3.9 MB/s eta 0:00:00\n",
      "Downloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.3.3 smart-open-7.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['word', 'embeddings', 'type', 'word', 'representation', 'allows', 'words', 'similar', 'meaning', 'similar', 'representation'], ['word2vec', 'two-layer', 'neural', 'network']]\n",
      "[ 8.13227147e-03 -4.45733406e-03 -1.06835726e-03  1.00636482e-03\n",
      " -1.91113955e-04  1.14817743e-03  6.11386076e-03 -2.02715401e-05\n",
      " -3.24596534e-03 -1.51072862e-03  5.89729892e-03  1.51410222e-03\n",
      " -7.24261976e-04  9.33324732e-03 -4.92128357e-03 -8.38409644e-04\n",
      "  9.17541143e-03  6.74942741e-03  1.50285603e-03 -8.88256077e-03\n",
      "  1.14874600e-03 -2.28825561e-03  9.36823711e-03  1.20992784e-03\n",
      "  1.49006362e-03  2.40640994e-03 -1.83600665e-03 -4.99963388e-03\n",
      "  2.32429506e-04 -2.01418041e-03  6.60093315e-03  8.94012302e-03\n",
      " -6.74754381e-04  2.97701475e-03 -6.10765442e-03  1.69932481e-03\n",
      " -6.92623248e-03 -8.69402662e-03 -5.90020278e-03 -8.95647518e-03\n",
      "  7.27759488e-03 -5.77203138e-03  8.27635173e-03 -7.24354526e-03\n",
      "  3.42167495e-03  9.67499893e-03 -7.78544787e-03 -9.94505733e-03\n",
      " -4.32914635e-03 -2.68313056e-03 -2.71289347e-04 -8.83155130e-03\n",
      " -8.61755759e-03  2.80021061e-03 -8.20640661e-03 -9.06933658e-03\n",
      " -2.34046578e-03 -8.63180775e-03 -7.05664977e-03 -8.40115082e-03\n",
      " -3.01328895e-04 -4.56429832e-03  6.62717456e-03  1.52716041e-03\n",
      " -3.34147573e-03  6.10897178e-03 -6.01328490e-03 -4.65616956e-03\n",
      " -7.20750913e-03 -4.33658017e-03 -1.80932996e-03  6.48964290e-03\n",
      " -2.77039292e-03  4.91896737e-03  6.90444233e-03 -7.46370573e-03\n",
      "  4.56485013e-03  6.12697843e-03 -2.95447465e-03  6.62502181e-03\n",
      "  6.12587947e-03 -6.44348515e-03 -6.76455162e-03  2.53895880e-03\n",
      " -1.62381888e-03 -6.06512791e-03  9.49920900e-03 -5.13014663e-03\n",
      " -6.55409694e-03 -1.19885204e-04 -2.70142802e-03  4.44400299e-04\n",
      " -3.53745813e-03 -4.19330609e-04 -7.08615757e-04  8.22820642e-04\n",
      "  8.19481723e-03 -5.73670724e-03 -1.65952800e-03  5.57160750e-03]\n",
      "[('words', 0.19951613247394562), ('embeddings', 0.17275308072566986), ('neural', 0.17015796899795532), ('word2vec', 0.14596611261367798), ('two-layer', 0.0640842393040657)]\n"
     ]
    }
   ],
   "source": [
    "#9. : How can you perform word embedding using Word2Vec?\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Sample text\n",
    "corpus = [\n",
    "    \"Word embeddings are a type of word representation that allows words with similar meaning to have a similar representation.\",\n",
    "    \"Word2Vec is a two-layer neural network.\"\n",
    "]\n",
    "\n",
    "# Tokenize and preprocess\n",
    "processed_corpus = []\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "for sentence in corpus:\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]\n",
    "    processed_corpus.append(tokens)\n",
    "\n",
    "print(processed_corpus)\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences=processed_corpus, vector_size=100, window=5, min_count=1, sg=1)  # sg=1 for Skip-Gram\n",
    "\n",
    "# Save the model\n",
    "model.save(\"word2vec.model\")\n",
    "\n",
    "# Load the model\n",
    "model = Word2Vec.load(\"word2vec.model\")\n",
    "\n",
    "# Get the vector for a word\n",
    "word_vector = model.wv[\"word2vec\"]  # Replace 'word2vec' with any word in your vocabulary\n",
    "print(word_vector)\n",
    "\n",
    "# Find similar words\n",
    "similar_words = model.wv.most_similar(\"word\", topn=5)\n",
    "print(similar_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.1869079e-03  1.4356810e-03 -7.0191367e-04  2.8874034e-03\n",
      " -3.4659742e-03 -2.7936087e-03 -1.0352881e-03 -2.7252214e-03\n",
      " -4.1071456e-03  1.7943596e-03  1.3235278e-04 -1.7138472e-03\n",
      " -5.1885014e-03 -2.0671624e-03 -3.4049493e-03  5.8595202e-04\n",
      " -1.8258099e-03 -5.8258040e-04 -1.5010035e-05 -5.2585220e-03\n",
      "  1.0673769e-03 -7.3448342e-04 -6.1709789e-04  4.4508246e-03\n",
      " -5.1288228e-03 -1.2450522e-03 -2.9584689e-03 -1.8150475e-03\n",
      " -4.2175758e-03  3.6633262e-04 -3.2818776e-03  1.6365347e-03\n",
      "  4.7468212e-03 -6.4397245e-03  2.7811602e-03 -3.5659429e-03\n",
      "  4.4500744e-03 -5.7846005e-03 -1.3143548e-03 -2.6828861e-03\n",
      " -2.5027157e-03  2.6898708e-03 -5.8403984e-04 -4.8826370e-04\n",
      "  1.2097222e-03 -1.9689337e-03  1.7114119e-03 -2.1633836e-03\n",
      " -1.2357481e-04 -4.6707401e-03  2.2150103e-03 -1.5692430e-03\n",
      " -1.4587210e-03 -5.1167859e-03  5.3673470e-05 -1.5675714e-03\n",
      "  3.4580785e-03  5.1236777e-03 -3.2806403e-03  1.9407374e-03\n",
      "  1.3701223e-03 -5.2977400e-04  6.5556285e-03  1.6704669e-03\n",
      " -2.7113240e-03  4.0116007e-03 -2.9830646e-03 -3.7055612e-03\n",
      " -4.1519562e-03 -4.9066488e-03 -3.2142214e-03  5.2592729e-04\n",
      " -5.3295423e-04  2.8320749e-03  2.2748145e-03 -4.1188612e-03\n",
      "  3.3041064e-03 -3.1705529e-03  3.8242072e-03 -3.9167195e-03\n",
      "  2.6603094e-03  3.9016572e-03  1.2086822e-03  3.7878612e-03\n",
      "  3.0858358e-03  1.9967563e-03  8.6543296e-04  2.2934484e-03\n",
      "  1.3496029e-03  1.0197900e-03 -5.3504352e-03  4.6459381e-03\n",
      " -2.4308856e-03 -2.5642978e-03  3.8684933e-03  8.8992197e-04\n",
      " -3.5666581e-03  3.2558632e-03  3.9246823e-03  5.0821798e-03]\n",
      "[('0', 0.326983243227005), ('2', 0.2665003836154938)]\n"
     ]
    }
   ],
   "source": [
    "#10: How can you use Doc2Vec to embed documents\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"Doc2Vec is a model that learns vector representations for entire documents.\",\n",
    "    \"It is an extension of the Word2Vec model.\",\n",
    "    \"Doc2Vec works by tagging each document with a unique identifier.\"\n",
    "]\n",
    "\n",
    "# Preprocess and tag the documents\n",
    "tagged_data = [TaggedDocument(words=word_tokenize(doc.lower()), tags=[str(i)]) for i, doc in enumerate(corpus)]\n",
    "\n",
    "# Train a Doc2Vec model\n",
    "model = Doc2Vec(vector_size=100, window=2, min_count=1, workers=4, epochs=100)\n",
    "\n",
    "# Build vocabulary from the tagged documents\n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "# Train the model\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# Now you can infer the vector for a new document (not part of the training corpus)\n",
    "new_doc = \"Doc2Vec generates fixed-length vectors for variable-length documents.\"\n",
    "\n",
    "# Tokenize the new document\n",
    "new_vector = model.infer_vector(word_tokenize(new_doc.lower()))\n",
    "\n",
    "# Print the inferred vector for the new document\n",
    "print(new_vector)\n",
    "\n",
    "# Find the most similar documents to the new document\n",
    "similar_docs = model.dv.most_similar(new_vector, topn=2)\n",
    "print(similar_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NLTK', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('leading', 'VBG'), ('platform', 'NN'), ('for', 'IN'), ('building', 'VBG'), ('Python', 'NNP'), ('programs', 'NNS'), ('to', 'TO'), ('work', 'VB'), ('with', 'IN'), ('human', 'JJ'), ('language', 'NN'), ('data', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "#11. : How can you perform part-of-speech tagging\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
    "\n",
    "# Tokenize and POS tagging\n",
    "tokens = word_tokenize(sentence)\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "print(pos_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.4112070550676187\n"
     ]
    }
   ],
   "source": [
    "#12. How can you find the similarity between two sentences using cosine similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Sample sentences\n",
    "sentence1 = \"I love machine learning\"\n",
    "sentence2 = \"Machine learning is great\"\n",
    "\n",
    "# Vectorize the sentences\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform([sentence1, sentence2])\n",
    "\n",
    "# Compute cosine similarity\n",
    "cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "\n",
    "print(f\"Cosine Similarity: {cosine_sim[0][0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Load pre-trained model\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men_core_web_sm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Sample sentence\u001b[39;00m\n\u001b[0;32m     10\u001b[0m sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApple is planning to open a new store in New York City.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages\\spacy\\__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages\\spacy\\util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "#13.  How can you extract named entities from a sentence\n",
    "\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Load pre-trained model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"Apple is planning to open a new store in New York City.\"\n",
    "\n",
    "# Process the sentence\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Extract Named Entities\n",
    "named_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "print(named_entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a large document that will be split into smaller chunks for processing.']\n"
     ]
    }
   ],
   "source": [
    "#14. How can you split a large document into smaller chunks of text\n",
    "# Sample large document\n",
    "document = \"This is a large document that will be split into smaller chunks for processing.\"\n",
    "\n",
    "# Split document into sentences\n",
    "sentences = document.split(\". \")\n",
    "\n",
    "# Print sentences\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.55847784 0.         0.43370786 0.55847784 0.\n",
      "  0.43370786]\n",
      " [0.         0.         0.55847784 0.43370786 0.         0.55847784\n",
      "  0.43370786]\n",
      " [0.40474829 0.30782151 0.30782151 0.47810172 0.30782151 0.30782151\n",
      "  0.47810172]]\n"
     ]
    }
   ],
   "source": [
    "#15. How can you calculate the TF-IDF (Term Frequency - Inverse Document Frequency) for a set of document\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\"The sky is blue.\", \"The sun is bright.\", \"The sky is blue and the sun is bright.\"]\n",
    "\n",
    "# Vectorize the documents using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Print the TF-IDF matrix\n",
    "print(tfidf_matrix.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['learn', 'natur', 'languag', 'process', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhin\\anaconda3\\envs\\assignment_env\\lib\\site-packages\\IPython\\core\\inputtransformer2.py:627: UserWarning: `make_tokens_by_line` received a list of lines which do not have lineending markers ('\\n', '\\r', '\\r\\n', '\\x0b', '\\x0c'), behavior will be unspecified\n",
      "  tokens_by_line = make_tokens_by_line(lines)\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\abhin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\abhin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#16. How can you apply tokenization, stopword removal, and stemming in one go\u001c\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"I am learning natural language processing.\"\n",
    "\n",
    "# Tokenize\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "# Stem the words\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "\n",
    "print(stemmed_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#17.  How can you visualize the frequency distribution of words in a sentence\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"This is a test sentence. This is another test sentence.\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "# Create frequency distribution\n",
    "fdist = FreqDist(tokens)\n",
    "\n",
    "# Plot the frequency distribution\n",
    "fdist.plot()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
